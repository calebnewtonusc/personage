# Ollama Configuration
# Use Ollama cloud (https://ollama.com) or local (http://localhost:11434)
OLLAMA_BASE_URL=https://ollama.com

# Model to use (e.g. gemma3:4b, llama3.2, mistral)
OLLAMA_MODEL=llama3.2

# API key â€” required for Ollama cloud, leave blank for local
OLLAMA_API_KEY=your_ollama_api_key_here
