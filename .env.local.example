# Ollama Configuration
# Point to your Ollama instance â€” local or any hosted Ollama endpoint
OLLAMA_BASE_URL=http://localhost:11434

# Model to use (must be pulled in your Ollama instance)
# Examples: llama3.2, llama3.1, mistral, gemma2
OLLAMA_MODEL=llama3.2
